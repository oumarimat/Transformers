{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0T9k3mjU3c-",
        "outputId": "53008f24-1fe9-4884-add4-744c7a1271cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objectif du projet:\n",
        "\n",
        "L'objectif de ce projet était de construire et d'entraîner un modèle de traduction automatique simple utilisant une architecture Transformer. Le modèle est conçu pour traduire de courtes phrases d'anglais vers le français en utilisant un petit ensemble de données d'exemples de traduction.\n",
        "\n",
        "Étapes réalisées :\n",
        "\n",
        "Installation des dépendances: Les bibliothèques nécessaires (torch, numpy, tqdm) ont été installées.\n",
        "Préparation des données:\n",
        "Un petit ensemble de paires de phrases anglais-français a été créé.\n",
        "Cet ensemble de données a été étendu par duplication et mélange pour obtenir environ 1000 exemples.\n",
        "Un SimpleTokenizer a été implémenté pour convertir les phrases en séquences d'identifiants de mots, incluant des tokens spéciaux (<pad>, <sos>, <eos>, <unk>).\n",
        "Une classe TranslationDataset a été définie pour préparer les données pour l'entraînement, y compris l'encodage des phrases et le padding à une longueur maximale.\n",
        "Un DataLoader a été créé pour charger les données par lots pendant l'entraînement.\n",
        "Définition du modèle: Un modèle Transformer simple a été défini à l'aide de PyTorch, comprenant des couches d'embedding, un encodage positionnel et des couches d'encodeur-décodeur Transformer standard.\n",
        "Entraînement du modèle:\n",
        "La fonction de perte CrossEntropyLoss et l'optimiseur Adam ont été configurés.\n",
        "Le modèle a été entraîné sur l'ensemble de données préparé pendant 20 époques.\n",
        "La perte d'entraînement a été suivie à chaque époque, montrant une diminution significative au fil du temps, ce qui indique que le modèle apprend.\n",
        "Résultats de l'entraînement :\n",
        "\n",
        "La perte d'entraînement a diminué de manière constante sur les 20 époques. La perte finale d'environ 0.0034 suggère que le modèle a bien appris l'ensemble de données limité et est capable de prédire les séquences cibles avec une grande précision sur cet ensemble de données.\n",
        "\n",
        "Inférence :\n",
        "\n",
        "Une fonction translate a été définie pour utiliser le modèle entraîné afin de générer des traductions pour de nouvelles phrases d'entrée. Les exemples de test montrés (\"Bonjour\" et \"Merci\") ont produit les traductions attendues (\"bonjour\" et \"bonjour\" - notez que la traduction de \"Merci\" semble erronée dans l'exemple fourni dans le notebook, ce qui pourrait indiquer une limitation du modèle sur un ensemble de données aussi petit).\n",
        "\n",
        "Prochaines étapes possibles :\n",
        "\n",
        "Utiliser un ensemble de données de traduction plus grand et plus diversifié pour améliorer les performances et la généralisation du modèle.\n",
        "Ajouter des mécanismes d'attention plus sophistiqués au modèle Transformer.\n",
        "Mettre en œuvre un processus de validation pour évaluer les performances du modèle sur des données invisibles pendant l'entraînement.\n",
        "Expérimenter différents hyperparamètres (taille du modèle, nombre de couches, taux d'apprentissage, etc.).\n",
        "Déployer le modèle pour une utilisation pratique.\n",
        "Ce rapport résume les principales composantes et les résultats de ce projet de traduction automatique.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9ai1bWqXfL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# tokenizer.py (Included for self-containment)\n",
        "from collections import defaultdict\n",
        "\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.idx_to_token = {}\n",
        "        self.token_to_idx = {}\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "\n",
        "    def fit(self, sentences):\n",
        "        vocab = set([self.pad_token, self.sos_token, self.eos_token, self.unk_token])\n",
        "        for sent in sentences:\n",
        "            vocab.update(sent.lower().split())\n",
        "        self.vocab = sorted(vocab)\n",
        "        self.idx_to_token = {i: token for i, token in enumerate(self.vocab)}\n",
        "        self.token_to_idx = {token: i for i, token in self.idx_to_token.items()} # Corrected typo here\n",
        "\n",
        "    def encode(self, sentence, add_special=True):\n",
        "        tokens = sentence.lower().split()\n",
        "        ids = [self.token_to_idx.get(t, self.token_to_idx[self.unk_token]) for t in tokens]\n",
        "        if add_special:\n",
        "            ids = [self.token_to_idx[self.sos_token]] + ids + [self.token_to_idx[self.eos_token]]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids, skip_special=True):\n",
        "        if skip_special:\n",
        "            ids = [i for i in ids if i not in [\n",
        "                self.token_to_idx[self.sos_token],\n",
        "                self.token_to_idx[self.eos_token],\n",
        "                self.token_to_idx[self.pad_token]\n",
        "            ]]\n",
        "        return \" \".join(self.idx_to_token.get(i, self.unk_token) for i in ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "\n",
        "# dataset.py (Included for self-containment)\n",
        "data = [\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"how are you\", \"comment vas-tu\"),\n",
        "    (\"i am fine\", \"je vais bien\"),\n",
        "    (\"good morning\", \"bonjour\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"what is your name\", \"comment tu t'appelles\"),\n",
        "    (\"see you later\", \"à plus tard\"),\n",
        "    (\"i love to code\", \"j'adore coder\"),\n",
        "    (\"this is a test\", \"c'est un test\"),\n",
        "    (\"machine learning is fun\", \"l'apprentissage automatique est amusant\"),\n",
        "]\n",
        "\n",
        "# On duplique pour avoir ~1000 exemples\n",
        "import random\n",
        "random.seed(42)\n",
        "extended_data = data * 100\n",
        "random.shuffle(extended_data)\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, src_tokenizer, tgt_tokenizer, max_len=20):\n",
        "        self.data = data\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        src_ids = self.src_tokenizer.encode(src)[:self.max_len]\n",
        "        tgt_ids = self.tgt_tokenizer.encode(tgt)[:self.max_len]\n",
        "\n",
        "        # Padding\n",
        "        src_ids = src_ids + [self.src_tokenizer.token_to_idx[\"<pad>\"]] * (self.max_len - len(src_ids))\n",
        "        tgt_ids = tgt_ids + [self.tgt_tokenizer.token_to_idx[\"<pad>\"]] * (self.max_len - len(tgt_ids))\n",
        "\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids[:-1]), torch.tensor(tgt_ids[1:])  # input, target_in, target_out\n",
        "\n",
        "# model.py (Included for self-containment)\n",
        "import math\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, max_len=50):\n",
        "        super().__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_embedding = self.create_positional_encoding(max_len, d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def create_positional_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.src_embedding(src) + self.pos_embedding[:, :src.size(1), :]\n",
        "        tgt = self.tgt_embedding(tgt) + self.pos_embedding[:, :tgt.size(1), :]\n",
        "\n",
        "        src = self.dropout(src)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        output = self.transformer(src, tgt)\n",
        "        return self.fc_out(output)\n",
        "\n",
        "\n",
        "# Tokenizers\n",
        "src_tokenizer = SimpleTokenizer()\n",
        "tgt_tokenizer = SimpleTokenizer()\n",
        "\n",
        "src_sentences = [src for src, tgt in extended_data]\n",
        "tgt_sentences = [tgt for src, tgt in extended_data]\n",
        "\n",
        "src_tokenizer.fit(src_sentences)\n",
        "tgt_tokenizer.fit(tgt_sentences)\n",
        "\n",
        "# Dataset & Dataloader\n",
        "dataset = TranslationDataset(extended_data, src_tokenizer, tgt_tokenizer, max_len=15)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Model\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(src_tokenizer),\n",
        "    tgt_vocab_size=len(tgt_tokenizer),\n",
        "    d_model=64,\n",
        "    nhead=4,\n",
        "    num_layers=2\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_tokenizer.token_to_idx[\"<pad>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(20):\n",
        "    total_loss = 0\n",
        "    for src, tgt_in, tgt_out in tqdm(dataloader, desc=f\"Epoch {epoch+1}/20\"):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_in)\n",
        "        loss = criterion(output.view(-1, len(tgt_tokenizer)), tgt_out.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3FliX8lV34J",
        "outputId": "c9132397-4323-44f3-86a5-2cfe0b7812e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 63/63 [00:02<00:00, 25.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.8610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 63/63 [00:01<00:00, 33.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 0.3897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 63/63 [00:01<00:00, 32.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 0.1331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 63/63 [00:01<00:00, 33.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 0.0722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 63/63 [00:01<00:00, 33.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 0.0464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 63/63 [00:02<00:00, 30.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 0.0323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 63/63 [00:02<00:00, 24.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 0.0248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 63/63 [00:01<00:00, 33.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 0.0190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 63/63 [00:01<00:00, 33.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 0.0151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 63/63 [00:01<00:00, 33.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.0127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 63/63 [00:01<00:00, 33.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Loss: 0.0106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 63/63 [00:01<00:00, 32.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Loss: 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 63/63 [00:02<00:00, 21.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Loss: 0.0076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 63/63 [00:01<00:00, 33.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Loss: 0.0070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 63/63 [00:01<00:00, 33.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Loss: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 63/63 [00:01<00:00, 34.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Loss: 0.0053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 63/63 [00:01<00:00, 33.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Loss: 0.0046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 63/63 [00:01<00:00, 33.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Loss: 0.0042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 63/63 [00:02<00:00, 23.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Loss: 0.0037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 63/63 [00:01<00:00, 31.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Loss: 0.0034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference.py\n",
        "def translate(model, sentence, src_tokenizer, tgt_tokenizer, max_len=15):\n",
        "    model.eval()\n",
        "    src_ids = torch.tensor([src_tokenizer.encode(sentence, add_special=True)]).to(next(model.parameters()).device)\n",
        "\n",
        "    tgt_ids = [tgt_tokenizer.token_to_idx[\"<sos>\"]]\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor([tgt_ids])\n",
        "        with torch.no_grad():\n",
        "            output = model(src_ids, tgt_tensor)\n",
        "        next_token = output[0, -1].argmax().item()\n",
        "        tgt_ids.append(next_token)\n",
        "        if next_token == tgt_tokenizer.token_to_idx[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    return tgt_tokenizer.decode(tgt_ids)\n",
        "\n",
        "# Test\n",
        "print(translate(model, \"Bonjour\", src_tokenizer, tgt_tokenizer))\n",
        "print(translate(model, \"Merci\", src_tokenizer, tgt_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzowgWvbV69q",
        "outputId": "aaad05f9-c293-4f27-d742-5f0021343433"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bonjour\n",
            "bonjour\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/oumarimat/Transformers"
      ],
      "metadata": {
        "id": "BrcYP-V3ZDk3"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}